{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain_teddynote import logging\n",
    "from models import sample_name_searcher, get_rag_instance\n",
    "\n",
    "from utils import load_config, save_output2json\n",
    "from prompt import load_system_prompt, load_invoke_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일 로드\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# API 키 가져오기\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# LangSmith 추적 기능을 활성화합니다. (선택적)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from langchain.tools import Tool\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import functools\n",
    "from retriever import get_retriever\n",
    "\n",
    "\n",
    "# 각 에이전트와 도구에 대한 다른 노드를 생성할 것입니다. 이 클래스는 그래프의 각 노드 사이에서 전달되는 객체를 정의합니다.\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    sender: str\n",
    "\n",
    "class NewMultiAgentRAG:\n",
    "    def __init__(\n",
    "        self, \n",
    "        file_folder:str=\"./data/raw\", \n",
    "        file_number:int=1, \n",
    "        chunk_size: int=500, \n",
    "        chunk_overlap: int=100, \n",
    "        search_k: int=10,       \n",
    "        system_prompt:str = None, \n",
    "        model_name:str=\"gpt-4o\",\n",
    "        save_graph_png:bool=False,\n",
    "    ):\n",
    "        ## 파일 명 설정\n",
    "        if file_number < 10:\n",
    "            file_name = f\"paper_00{file_number}\"\n",
    "        elif file_number < 100:\n",
    "            file_name = f\"paper_0{file_number}\"\n",
    "        else:\n",
    "            file_name = f\"paper_{file_number}\"\n",
    "\n",
    "        ## retriever 설정\n",
    "        self.retriever = get_retriever(\n",
    "            file_folder=file_folder, \n",
    "            file_name=file_name, \n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap, \n",
    "            search_k=search_k\n",
    "        )\n",
    "        self.retriever_tool = Tool(\n",
    "            name=\"retriever\",\n",
    "            func=self.retriever.get_relevant_documents,\n",
    "            description=\"Retrieve relevant documents based on a query.\"\n",
    "        )\n",
    "\n",
    "        ## Coordinator Agent\n",
    "        self.coordinator_system_prompt = system_prompt[\"coordinator_system_prompt\"]\n",
    "\n",
    "        ## researcher 시스템 프롬프트\n",
    "        self.researcher_system_prompt = system_prompt[\"researcher_system_prompt\"]\n",
    "\n",
    "        ## verifier 시스템 프롬프트\n",
    "        self.verifier_system_prompt = \"\"\"You are a meticulous verifier agent specializing in the domain of battery technology.\n",
    "Your primary task is to verify the accuracy of the Researcher's answers by using the search tool to cross-check the extracted information from research papers on batteries, formatted into JSON.  \n",
    "\n",
    "Your responsibilities include validating the following:  \n",
    "\n",
    "### Accuracy:  \n",
    "Extracted values through documents retrieved via the search tool must be verified to ensure they match accurately.\n",
    "\n",
    "### Completeness:  \n",
    "Confirm that all fields in the JSON structure are either filled with accurate values from the battery-related sections of the PDF or marked as \"None\" if not mentioned in the document.  \n",
    "\n",
    "If any field is missing or only partially extracted, explicitly state:  \n",
    "- **Which fields are incomplete or missing**  \n",
    "- **Whether the missing information exists in the PDF but was not extracted, or is genuinely absent**  \n",
    "- **Suggestions for improvement (e.g., re-extraction, manual verification, or alternative sources if applicable)**  \n",
    "\n",
    "### Consistency:  \n",
    "Verify that the JSON structure, format, and data types adhere strictly to the required schema for battery-related research data.  \n",
    "\n",
    "### Corrections:  \n",
    "Identify and highlight any errors, including:  \n",
    "- **Inaccurate values** (i.e., extracted values that do not match the PDF)  \n",
    "- **Missing data** (i.e., fields left empty when information is available)  \n",
    "- **Formatting inconsistencies** (i.e., data types or schema mismatches)  \n",
    "\n",
    "For any issues found, provide a **clear and actionable correction**, including:  \n",
    "- **The specific field in question**  \n",
    "- **The nature of the issue (incorrect value, missing data, formatting error, etc.)**  \n",
    "- **Suggestions or corrections to resolve the issue**  \n",
    "\n",
    "### Handling Missing Data:  \n",
    "If certain information is genuinely **not found** in the PDF, specify:  \n",
    "- **Which fields could not be located**  \n",
    "- **Confirmation that they are absent from the document**  \n",
    "- **A recommendation to keep the field as `\"None\"` or any alternative solutions**  \n",
    "\n",
    "### Final Output:  \n",
    "If the JSON is entirely correct, confirm its validity and output the JSON structure exactly as provided.  \n",
    "Include the phrase `### Final Output` before printing the JSON. This ensures the output is clearly marked and easy to locate.  \n",
    "\n",
    "### Scope:  \n",
    "Focus **exclusively** on battery-related content extracted from the PDF.  \n",
    "Ignore any reference content or information outside the provided document.  \n",
    "\"\"\"\n",
    "        \n",
    "        ## agent 및 node 생성\n",
    "        self.model_name = model_name\n",
    "        llm = ChatOpenAI(model=self.model_name, temperature=0.1)\n",
    "\n",
    "        # Research agent and node\n",
    "        self.research_agent = self.create_agent(\n",
    "            llm,\n",
    "            [self.retriever_tool],\n",
    "            system_message=self.researcher_system_prompt,\n",
    "        )\n",
    "        self.research_node = functools.partial(self.agent_node, agent=self.research_agent, name=\"Researcher\")\n",
    "\n",
    "        # Data_Verifier\n",
    "        self.verifier_agent = self.create_agent(\n",
    "            llm,\n",
    "            [self.retriever_tool],\n",
    "            system_message=self.verifier_system_prompt,\n",
    "        )\n",
    "        self.verifier_node = functools.partial(self.agent_node, agent=self.verifier_agent, name=\"Data_Verifier\")\n",
    "\n",
    "        # Json_Processor\n",
    "        self.json_processor_system_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"You are a JSON Processor Agent. Your sole responsibility is to process the response generated by an LLM and ensure the accurate extraction of the JSON content within the response. Follow these instructions precisely:\n",
    "\n",
    "### Instructions:\n",
    "1. **Extract JSON Only**:\n",
    "- Identify the ```json``` block within the provided response.\n",
    "- Extract and output the content within the ```json``` block exactly as it appears.\n",
    "\n",
    "2. **No Modifications**:\n",
    "- Do not modify, add, or remove any part of the JSON content.\n",
    "- Preserve the relevancerag structure, field names, and values without alteration.\n",
    "\n",
    "3. **No Hallucination**:\n",
    "- Do not interpret, infer, or generate additional content.\n",
    "\n",
    "4. **Output Format**:\n",
    "- Respond with the extracted JSON content only.\n",
    "- Do not include any explanations, comments, or surrounding text.\n",
    "- The output must be a clean, valid JSON.\n",
    "\n",
    "### Your Role:\n",
    "Ensure the integrity and consistency of the JSON data by strictly adhering to these instructions. Your output should always be concise and compliant with the above rules.\"\"\"\n",
    "                ),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.json_processor_agent = self.json_processor_system_prompt | ChatOpenAI(model=self.model_name, temperature=0.1) | JsonOutputParser()\n",
    "        self.json_processor_node = functools.partial(self.json_processor_agent_node, agent=self.json_processor_agent, name=\"Json_Processor\")\n",
    "\n",
    "        self.tools = [self.retriever_tool]\n",
    "        self.tool_executor = ToolExecutor(self.tools)\n",
    "        \n",
    "\n",
    "        ## graph 구축\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        workflow.add_node(\"Researcher\", self.research_node)\n",
    "        workflow.add_node(\"Data_Verifier\", self.verifier_node)\n",
    "        workflow.add_node(\"call_tool\", self.tool_node)\n",
    "        workflow.add_node(\"Json_Processor\", self.json_processor_node)\n",
    "\n",
    "        workflow.add_edge(\"Json_Processor\", END)\n",
    "        workflow.add_conditional_edges(\n",
    "            \"Researcher\",\n",
    "            self.router,\n",
    "            {\"continue\": \"Data_Verifier\", \"call_tool\": \"call_tool\"},\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"Data_Verifier\",\n",
    "            self.router,\n",
    "            {\"continue\": \"Researcher\", \"call_tool\": \"call_tool\", \"process_output\": \"Json_Processor\"},\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"call_tool\",\n",
    "            lambda x: x[\"sender\"],\n",
    "            {\n",
    "                \"Researcher\": \"Researcher\",\n",
    "                \"Data_Verifier\": \"Data_Verifier\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        workflow.set_entry_point(\"Researcher\")\n",
    "        self.graph = workflow.compile()   \n",
    "        \n",
    "        if save_graph_png:        \n",
    "            self.graph.get_graph().draw_mermaid_png(output_file_path=\"./graph_img/multiagentrag_graph.png\")\n",
    "\n",
    "\n",
    "    def create_agent(self, llm, tools, system_message: str):\n",
    "        # 에이전트를 생성합니다.\n",
    "        functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                    \" Use the provided tools to progress towards answering the question.\"                                        \n",
    "                    \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                    \" will help where you left off. Execute what you can to make progress.\"\n",
    "                    \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                    \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "                ),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            ]\n",
    "        )\n",
    "        prompt = prompt.partial(system_message=system_message)\n",
    "        prompt = prompt.partial(tool_names=\", \".join(\n",
    "            [tool.name for tool in tools]))\n",
    "        return prompt | llm.bind_functions(functions)\n",
    "    \n",
    "    \n",
    "    def agent_node(self, state, agent, name):\n",
    "        result = agent.invoke(state)\n",
    "        if isinstance(result, FunctionMessage):\n",
    "            pass\n",
    "        else:\n",
    "            result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "        return {\n",
    "            \"messages\": [result],\n",
    "            \"sender\": name,\n",
    "        }\n",
    "\n",
    "\n",
    "    def json_processor_agent_node(self, state, agent, name):\n",
    "        result = agent.invoke(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=f\"\"\"Convert Final Output in the given response into a JSON format.: {state[\"messages\"][-1].content}\"\"\")\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        return {\"messages\": result, \"name\": name}\n",
    "\n",
    "\n",
    "    def tool_node(self, state):\n",
    "        # 그래프에서 도구를 실행하는 함수입니다.\n",
    "        # 에이전트 액션을 입력받아 해당 도구를 호출하고 결과를 반환합니다.\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # 계속 조건에 따라 마지막 메시지가 함수 호출을 포함하고 있음을 알 수 있습니다.\n",
    "        first_message = messages[0]\n",
    "        last_message = messages[-1]\n",
    "        \n",
    "        # ToolInvocation을 함수 호출로부터 구성합니다.\n",
    "        tool_input = json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "        tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
    "        \n",
    "        if tool_name == \"retriever\":\n",
    "            base_query = tool_input.get(\"__arg1\", \"\")  # 기존 query 가져오기\n",
    "            refined_query = f\"Context: {first_message.content} | Query: {base_query}\"\n",
    "            tool_input[\"__arg1\"] = refined_query\n",
    "        \n",
    "        # 단일 인자 입력은 값으로 직접 전달할 수 있습니다.\n",
    "        if len(tool_input) == 1 and \"__arg1\" in tool_input:\n",
    "            tool_input = next(iter(tool_input.values()))\n",
    "        \n",
    "        action = ToolInvocation(\n",
    "            tool=tool_name,\n",
    "            tool_input=tool_input,\n",
    "        )\n",
    "        \n",
    "        # 도구 실행자를 호출하고 응답을 받습니다.\n",
    "        response = self.tool_executor.invoke(action)\n",
    "        \n",
    "        # 응답을 사용하여 FunctionMessage를 생성합니다.\n",
    "        function_message = FunctionMessage(\n",
    "            content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "        )\n",
    "        \n",
    "        # 기존 리스트에 추가될 리스트를 반환합니다.\n",
    "        return {\"messages\": [function_message]}\n",
    "    \n",
    "    \n",
    "    def router(self, state):\n",
    "        # 상태 정보를 기반으로 다음 단계를 결정하는 라우터 함수\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        if \"function_call\" in last_message.additional_kwargs:\n",
    "            # 이전 에이전트가 도구를 호출함\n",
    "            return \"call_tool\"\n",
    "        if \"Final Output\" in last_message.content:\n",
    "            # 어느 에이전트든 작업이 끝났다고 결정함\n",
    "            return \"process_output\"\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    data_folder:str=\"./data\",\n",
    "    file_num_list:list=[11],\n",
    "    category_number:int=1, \n",
    "    chunk_size:int=500, \n",
    "    chunk_overlap:int=100, \n",
    "    search_k:int=10,       \n",
    "    config_folder:str=\"./config\",\n",
    "    rag_method:str=\"multiagent-rag\", \n",
    "    model_name:str=\"gpt-4o\", \n",
    "    save_graph_png:bool=False, \n",
    "):\n",
    "    category_names = [\"CAM (Cathode Active Material)\", \"Electrode (half-cell)\", \"Morphological Properties\", \"Cathode Performance\"]\n",
    "    \n",
    "    ## system_prompt 와 invoke_input 불러오기\n",
    "    system_prompt = load_system_prompt(config_folder=config_folder, category_number=category_number, rag_method=rag_method)\n",
    "    invoke_input = load_invoke_input(config_folder=config_folder, category_number=category_number, rag_method=rag_method)\n",
    "    \n",
    "    total_answer = []\n",
    "    \n",
    "    ## 각 논문에 대해 반복\n",
    "    for file_number in file_num_list:\n",
    "        print(f\"#####    {file_number}번째 논문    #####\")\n",
    "        print(f\"##       rag method     : {rag_method}\")\n",
    "        print(f\"##       category name  : {category_names[category_number-1]}\")\n",
    "        \n",
    "        ## graph 호출\n",
    "        voltai_graph = get_rag_instance(\n",
    "            rag_method=rag_method, \n",
    "            file_folder=f\"{data_folder}/input_data/\", \n",
    "            file_number=file_number, \n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap, \n",
    "            search_k=search_k, \n",
    "            system_prompt=system_prompt,\n",
    "            model_name=model_name, \n",
    "            save_graph_png=save_graph_png,\n",
    "        ).graph\n",
    "        \n",
    "        ## 질문이 딕셔너리 형태일 경우와 아닌 경우를 처리\n",
    "        if isinstance(invoke_input, dict):\n",
    "            result = voltai_graph.invoke(**invoke_input)\n",
    "        else:\n",
    "            result = voltai_graph.invoke(*invoke_input)\n",
    "\n",
    "        ## RAG method에 따른 결과 확인\n",
    "        if result.get(\"answer\"):\n",
    "            temp_answer = result[\"answer\"][0][category_names[category_number-1]]\n",
    "        elif result.get(\"discussion\"):\n",
    "            temp_answer = result[\"discussion\"][category_names[category_number-1]]\n",
    "        elif result.get(\"messages\"):\n",
    "            temp_answer = result[\"messages\"][-1][category_names[category_number-1]]\n",
    "        \n",
    "        print(f\"##       print {file_number} result\")\n",
    "        print(\"------------------------------------\")\n",
    "        pprint(temp_answer, sort_dicts=False)\n",
    "        \n",
    "        ## json 저장하는 코드\n",
    "        save_output2json(each_answer=temp_answer,file_num=file_number, rag_method=rag_method, category_number=category_number)\n",
    "        \n",
    "        total_answer.append(temp_answer)\n",
    "        \n",
    "    return total_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num_list = [39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiagent RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##       ./config/multiagent-rag/c1-system-prompt.yaml를 불러왔습니다.\n",
      "##       ./config/multiagent-rag/c1-question.yaml를 불러왔습니다.\n",
      "#####    11번째 논문    #####\n",
      "##       rag method     : multiagent-rag\n",
      "##       category name  : CAM (Cathode Active Material)\n",
      "##       paper_011 retriever를 생성했습니다.\n",
      "##          - chunk_size    :500\n",
      "##          - chunk_overlap :100\n",
      "##          - retrieve_k    :10\n",
      "##       print 11 result\n",
      "------------------------------------\n",
      "{'Stoichiometry information': {'LiNi1/3Co1/3Mn1/3O2': {'Li ratio': 1.0,\n",
      "                                                       'Ni ratio': 0.33,\n",
      "                                                       'Co ratio': 0.33,\n",
      "                                                       'Mn ratio': 0.33,\n",
      "                                                       'O ratio': 2.0}},\n",
      " 'Commercial NCM used': {'LiNi1/3Co1/3Mn1/3O2': 'no'},\n",
      " 'Lithium source': 'LiNO3',\n",
      " 'Synthesis method': 'solution combustion',\n",
      " 'Crystallization method': 'calcination',\n",
      " 'Crystallization final temperature': 850,\n",
      " 'Crystallization final duration (hours)': 15,\n",
      " 'Doping': 'None',\n",
      " 'Coating': 'RGO',\n",
      " 'Additional treatment': 'microwave irradiation'}\n"
     ]
    }
   ],
   "source": [
    "multiagent_rag_c1_answer = main(file_num_list=file_num_list, category_number=1, rag_method=\"multiagent-rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##       ./config/multiagent-rag/c2-system-prompt.yaml를 불러왔습니다.\n",
      "##       ./config/multiagent-rag/c2-question.yaml를 불러왔습니다.\n",
      "#####    11번째 논문    #####\n",
      "##       rag method     : multiagent-rag\n",
      "##       category name  : Electrode (half-cell)\n",
      "##       paper_011 retriever를 생성했습니다.\n",
      "##          - chunk_size    :500\n",
      "##          - chunk_overlap :100\n",
      "##          - retrieve_k    :10\n",
      "##       print 11 result\n",
      "------------------------------------\n",
      "{'Active material to Conductive additive to Binder ratio': '87:3:10',\n",
      " 'Electrolyte': [{'Salt': 'LiPF6',\n",
      "                  'Concentration': '1M',\n",
      "                  'Solvent': 'EC:DMC',\n",
      "                  'Solvent ratio': '1:1'}],\n",
      " 'Additive': 'RGO, 5%',\n",
      " 'Loading density (mass loading of NCM)': 'None',\n",
      " 'Additional treatment for electrode': 'None'}\n"
     ]
    }
   ],
   "source": [
    "# multiagent_rag_c2_answer = main(file_num_list=file_num_list, category_number=2, rag_method=\"multiagent-rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ./config/multiagent-rag/c3-system-prompt.yaml를 불러왔습니다.\n",
      "## ./config/multiagent-rag/c3-question.yaml를 불러왔습니다.\n",
      "#####    11번째 논문    #####\n",
      "##       rag method     : multiagent-rag\n",
      "##       category name  : Morphological Properties\n",
      "##       paper_011 retriever를 생성했습니다.\n",
      "##          - chunk_size    :500\n",
      "##          - chunk_overlap :100\n",
      "##          - retrieve_k    :10\n",
      "##       print 11 result\n",
      "------------------------------------\n",
      "{'ParticleSize': {'NCM': '200-300 nm'},\n",
      " 'ParticleShape': {'NCM': 'faceted morphology, regular polyhedrons with smooth '\n",
      "                          'surfaces'},\n",
      " 'ParticleDistribution': {'NCM': 'uniform distribution with no significant '\n",
      "                                 'agglomeration'},\n",
      " 'CoatingLayerCharacteristics': {'NCMeRGO': 'RGO sheets wrapped around NCM '\n",
      "                                            'nanoparticles forming a '\n",
      "                                            'core-shell-like structure'},\n",
      " 'CrystalStructureAndLatticeCharacteristics': {'NCM': 'hexagonal structure '\n",
      "                                                      'with principal '\n",
      "                                                      'crystallographic planes '\n",
      "                                                      '(003) and (104), '\n",
      "                                                      'I(003)/I(104) ratio of '\n",
      "                                                      '1.36, c/a ratio of '\n",
      "                                                      '4.963'}}\n",
      "##       ./output/json/multiagent-rag/category3/paper_011_output-250203151025.json를 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "multiagent_rag_c3_answer = main(file_num_list=file_num_list, category_number=3, rag_method=\"multiagent-rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ./config/multiagent-rag/c4-system-prompt.yaml를 불러왔습니다.\n",
      "## ./config/multiagent-rag/c4-question.yaml를 불러왔습니다.\n",
      "#####    11번째 논문    #####\n",
      "##       rag method     : multiagent-rag\n",
      "##       category name  : Cathode Performance\n",
      "##       paper_011 retriever를 생성했습니다.\n",
      "##          - chunk_size    :500\n",
      "##          - chunk_overlap :100\n",
      "##          - retrieve_k    :10\n",
      "##       print 11 result\n",
      "------------------------------------\n",
      "{'NR0': [{'Voltage range': '2.5 - 4.3',\n",
      "          'Temperature': 25,\n",
      "          'C-rate and Specific capacity': [{'C-rate': 0.1, 'Capacity': 155.3},\n",
      "                                           {'C-rate': 0.2, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 0.5, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 1.0, 'Capacity': 123},\n",
      "                                           {'C-rate': 2.0, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 4.0, 'Capacity': 'None'},\n",
      "                                           {'Other C-rates and performance': [{'C-rate': 5.0,\n",
      "                                                                               'Capacity': 45.5}]}]}],\n",
      " 'NR1': [{'Voltage range': '2.5 - 4.3',\n",
      "          'Temperature': 25,\n",
      "          'C-rate and Specific capacity': [{'C-rate': 0.1, 'Capacity': 154.6},\n",
      "                                           {'C-rate': 0.2, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 0.5, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 1.0, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 2.0, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 4.0, 'Capacity': 'None'},\n",
      "                                           {'Other C-rates and performance': [{'C-rate': 5.0,\n",
      "                                                                               'Capacity': 47.0}]}]}],\n",
      " 'NR3': [{'Voltage range': '2.5 - 4.3',\n",
      "          'Temperature': 25,\n",
      "          'C-rate and Specific capacity': [{'C-rate': 0.1, 'Capacity': 166.5},\n",
      "                                           {'C-rate': 0.2, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 0.5, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 1.0, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 2.0, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 4.0, 'Capacity': 'None'},\n",
      "                                           {'Other C-rates and performance': [{'C-rate': 5.0,\n",
      "                                                                               'Capacity': 75.4}]}]}],\n",
      " 'NR5': [{'Voltage range': '2.5 - 4.3',\n",
      "          'Temperature': 25,\n",
      "          'C-rate and Specific capacity': [{'C-rate': 0.1, 'Capacity': 175},\n",
      "                                           {'C-rate': 0.2, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 0.5, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 1.0, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 2.0, 'Capacity': 'None'},\n",
      "                                           {'C-rate': 4.0, 'Capacity': 'None'},\n",
      "                                           {'Other C-rates and performance': [{'C-rate': 5.0,\n",
      "                                                                               'Capacity': 95.4}]}]}]}\n",
      "##       ./output/json/multiagent-rag/category4/paper_011_output-250203152049.json를 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "multiagent_rag_c4_answer = main(file_num_list=file_num_list, category_number=4, rag_method=\"multiagent-rag\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voltai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
