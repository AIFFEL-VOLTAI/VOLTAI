import warnings
import json
import google.generativeai as genai
warnings.filterwarnings("ignore")

from langgraph.graph import END, StateGraph
from langchain_core.output_parsers import JsonOutputParser

from typing import Annotated, TypedDict
#from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from typing_extensions import TypedDict

from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.documents.base import Document
from langchain_teddynote.messages import messages_to_history
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field

from retriever import get_retriever


# GraphState ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    question: Annotated[str, "Question"]  # ì§ˆë¬¸
    context: Annotated[str, "Context"]  # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
    example: Annotated[dict, "Example"] # ì˜ˆì‹œ
    answer: Annotated[str, "Answer"]  # ë‹µë³€
    messages: Annotated[list, add_messages]  # ë©”ì‹œì§€(ëˆ„ì ë˜ëŠ” list)

# Graph êµ¬ì¶•
class RelevanceRAG:
    def __init__(
        self, 
        file_folder:str="./data/input_data/", 
        file_number:int=1, 
        # db_folder:str="./vectordb", 
        chunk_size: int=500, 
        chunk_overlap: int=100, 
        search_k: int=10,       
        system_prompt:str = None, 
        model_name:str="gemini-3-pro-preview",
        save_graph_png:bool=False,
    ):
        self.retriever = get_retriever(
            file_folder=file_folder, 
            file_number=file_number, 
            chunk_size=chunk_size, 
            chunk_overlap=chunk_overlap, 
            search_k=search_k
        )
        self.model_name = model_name

        # =======================================================
        # ğŸŒŸ [ì¶”ê°€ëœ ì½”ë“œ] generation_config ë³€ìˆ˜ ì •ì˜ ğŸŒŸ
        # * NameError í•´ê²° ë° RAG ìµœì í™” ì„¤ì • ì ìš©
        # =======================================================
        generation_config = {
            "temperature": 1.0,          
            "max_output_tokens": 2048,     # JSON ì¶œë ¥ì„ ìœ„í•´ ë„‰ë„‰íˆ ì„¤ì •
            "thinking_config": {
                "include_thoughts": False, # LangGraphì˜ JSON íŒŒì‹± ì˜¤ë¥˜ ë°©ì§€ (í•„ìˆ˜)
                "thinking_level": "high"    # ì†ë„ ê°œì„ ì„ ìœ„í•´ Low Reasoning ë ˆë²¨ë¡œ ì„¤ì •
            }
        }
        # =======================================================

        self.model = ChatGoogleGenerativeAI(model=self.model_name, generation_config=generation_config) # ğŸ‘ˆ ì„¤ì • ì ìš©)
        self.relevance_checker = ChatGoogleGenerativeAI(model=self.model_name, temperature=1.0)
        self.llm_answer_prompt = system_prompt["llm_answer_system_prompt"]
        self.relevance_check_template = """
        You are a grader assessing relevance of a retrieved document to a user question. \n 
        Here is the retrieved document: \n\n {context} \n\n
        Here is the answer: {answer} \n
        If the document contains keyword(s) or semantic meaning related to the user answer, grade it as relevant. \n
        
        Give a binary score 'yes' or 'no' score to indicate whether the retrieved document is relevant to the answer.
        If the retrieved document does not contain the values or information being searched for, and 'None' is provided as the answer, check if the response accurately reflects the absence of the requested information. If the absence is accurate and justified, grade the document as relevant even if the values are 'None'.
        """
        
        # ê·¸ë˜í”„ ìƒì„±
        bulider = StateGraph(GraphState)

        # ë…¸ë“œ ì •ì˜
        bulider.add_node("retrieve", self.retrieve_document)
        bulider.add_node("relevance_check", self.relevance_check)
        bulider.add_node("llm_answer", self.llm_answer)

       # ì—£ì§€ ì •ì˜
        bulider.add_edge("retrieve", "llm_answer")  # _start_ -> ê²€ìƒ‰ ì‹œì‘
        bulider.add_edge("llm_answer", "relevance_check")  # ë‹µë³€ ìƒì„± -> ê´€ë ¨ì„± ì²´í¬

        # ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        bulider.add_conditional_edges(
            "relevance_check",  # ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ is_relevant í•¨ìˆ˜ì— ì „ë‹¬í•©ë‹ˆë‹¤.
            self.is_relevant,
            {
                "yes": END,  # ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ _end_ë¡œ ì´ë™í•©ë‹ˆë‹¤.
                "no": "retrieve",  # ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ë‹¤ì‹œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            },
        )

        # ê·¸ë˜í”„ ì§„ì…ì  ì„¤ì •
        bulider.set_entry_point("retrieve")
        
        # ì²´í¬í¬ì¸í„° ì„¤ì •
        memory = MemorySaver()

        # ì»´íŒŒì¼
        self.graph = bulider.compile(checkpointer=memory) 
        
        if save_graph_png:       
            self.graph.get_graph().draw_mermaid_png(output_file_path="./graph_img/relevancerag_graph.png")

    
    def format_docs(self, docs: list[Document]) -> str:
        """ë¬¸ì‹œ ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìë¡œ í•©ì¹˜ëŠ” ê¸°ëŠ¥ì„ í•©ë‹ˆë‹¤.

        Args:
            docs (list[Document]): ì—¬ëŸ¬ ê°œì˜ Documnet ê°ì²´ë¡œ ì´ë£¨ì–´ì§„ ë¦¬ìŠ¤íŠ¸

        Returns:
            str: ëª¨ë“  ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ê°€ í•˜ë‚˜ë¡œ í•©ì³ì§„ ë¬¸ìì—´ì„ ë°˜í™˜
        """
        return "\n\n".join(doc.page_content for doc in docs)
    
    
    def retrieve_document(self, state: GraphState) -> GraphState:
        """ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            state (GraphState): ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.

        Returns:
            GraphState: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•œ ìƒíƒœ ë³€ìˆ˜
        """        
        # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        latest_question = state["question"]

        # ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        retrieved_docs = self.retriever.invoke(latest_question)

        # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤.(í”„ë¡¬í”„íŠ¸ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê¸° ìœ„í•¨)
        retrieved_docs = self.format_docs(retrieved_docs)

        # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤.
        return GraphState(context=retrieved_docs)
    
    
    def llm_answer(self, state: GraphState) -> GraphState:
        """í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ LLMì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤. 

        Args:
            state (GraphState): ì§ˆë¬¸, ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤. 

        Returns:
            GraphState: json í˜•íƒœë¡œ ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸, ë‹µë³€) ë©”ì„¸ì§€ë¥¼ ì €ì¥í•œ ìƒíƒœ ë³€ìˆ˜
        """        
        # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        latest_question = state["question"]

        # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        context = state["context"]
        
        # example 
        example = state["example"]

        # prompt ì„¤ì •
        prompt = PromptTemplate(
            template=self.llm_answer_prompt,
            input_variables=["example", "context", "question"],
            )

        # ì²´ì¸ í˜¸ì¶œ
        chain = prompt | self.model | JsonOutputParser()

        response = chain.invoke(
            {
                "question": latest_question,
                "context": context, 
                "example": example,
                "chat_history": messages_to_history(state["messages"]),
            }
        )

        # ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸, ë‹µë³€) ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.

        if isinstance(response, list):
            final_answer = response
        else:
            final_answer = [response]

        return GraphState(
            answer=final_answer, 
            messages=[
                ("user", latest_question), 
                ("assistant", json.dumps(response, ensure_ascii=False)) 
            ]
        )


    def relevance_check(self, state: GraphState) -> GraphState:
        """ë‹µë³€ê³¼ ê²€ìƒ‰ ë¬¸ì„œ ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. 

        Args:
            state (GraphState): ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ë‹µë³€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. 

        Returns:
            GraphState: ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ì €ì¥í•œ ìƒíƒœ ë³€ìˆ˜
        """    
        
        class GradeAnswer(BaseModel):
            """Binary scoring to evaluate the appropriateness of answers to retrieval"""

            binary_score: str = Field(
                description="Indicate 'yes' or 'no' whether the answer solves the question"
            )
            
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = PromptTemplate(
            template=self.relevance_check_template,
            input_variables=["context", "answer"],
        )

        # ì²´ì¸
        structured_relevance_checker = self.relevance_checker.with_structured_output(GradeAnswer)
        relevance_chain = prompt | structured_relevance_checker
        
        # retrieval_answer_relevant = GroundednessChecker(
        #     llm=self.relevance_checker, target="retrieval-answer"
        # ).create()

        # ê´€ë ¨ì„± ì²´í¬ë¥¼ ì‹¤í–‰("yes" or "no")
        response = relevance_chain.invoke(
            {"context": state["context"], "answer": state["answer"]}
        )

        print(f"        RELEVANCE CHECK : {response.binary_score}")

        # ì°¸ê³ : ì—¬ê¸°ì„œì˜ ê´€ë ¨ì„± í‰ê°€ê¸°ëŠ” ê°ìì˜ Prompt ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ë“¤ì˜ Groundedness Check ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•´ ë³´ì„¸ìš”!
        return GraphState(relevance=response.binary_score)


    def is_relevant(self, state: GraphState) -> GraphState:
        """ê´€ë ¨ì„±ì„ ì²´í¬í•˜ëŠ” í•¨ìˆ˜

        Args:
            state (GraphState):

        Returns:
            GraphState: ê´€ë ¨ì„±ì„ ì €ì¥í•œ ìƒíƒœ ë³€ìˆ˜
        """        
        return state["relevance"]