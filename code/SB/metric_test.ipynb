{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROUGE ===\n",
      "{'rouge1': 0.15384615384615385, 'rouge2': 0.0, 'rougeL': 0.15384615384615385}\n",
      "\n",
      "=== BLEU ===\n",
      "BLEU Score: 5.791739854583281e-155\n",
      "\n",
      "=== METEOR ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): hi , i ' m teddy . nice to meet you !",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nz/glbk0zl17d35x39jy85r5vhc0000gn/T/ipykernel_38035/3716530000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== METEOR ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"METEOR Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_meteor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Semantic Similarity (STS) ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/voltai/code/SB/metrics/meteor_metric.py\u001b[0m in \u001b[0;36mcalculate_meteor\u001b[0;34m(question, answer)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_loaded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_teddynote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkiwi_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKiwiTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 토크나이저 선언\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mkiwi_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKiwiTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltai/lib/python3.11/site-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36mmeteor_score\u001b[0;34m(references, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mMETEOR\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     return max(\n\u001b[0m\u001b[1;32m    398\u001b[0m         single_meteor_score(\n\u001b[1;32m    399\u001b[0m             \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltai/lib/python3.11/site-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m    397\u001b[0m     return max(\n\u001b[0;32m--> 398\u001b[0;31m         single_meteor_score(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltai/lib/python3.11/site-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36msingle_meteor_score\u001b[0;34m(reference, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mMETEOR\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \"\"\"\n\u001b[0;32m--> 326\u001b[0;31m     enum_hypothesis, enum_reference = _generate_enums(\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/voltai/lib/python3.11/site-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m_generate_enums\u001b[0;34m(hypothesis, reference, preprocess)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;34mf'\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): {hypothesis}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): hi , i ' m teddy . nice to meet you !"
     ]
    }
   ],
   "source": [
    "# SB/metrics 내의 평가지표 모듈 임포트\n",
    "from metrics.rouge_metric import calculate_rouge\n",
    "from metrics.bleu_metric import calculate_bleu\n",
    "from metrics.meteor_metric import calculate_meteor\n",
    "from metrics.sem_score_metric import calculate_semantic_similarity\n",
    "\n",
    "# 테스트용 데이터 정의\n",
    "sent1 = \"Hello, my name is Teddy.\"\n",
    "sent2 = \"Hi, I'm Teddy. Nice to meet you!\"\n",
    "\n",
    "# 각 평가지표 테스트\n",
    "print(\"=== ROUGE ===\")\n",
    "print(calculate_rouge(sent1, sent2))\n",
    "\n",
    "print(\"\\n=== BLEU ===\")\n",
    "print(\"BLEU Score:\", calculate_bleu(sent1, sent2))\n",
    "\n",
    "print(\"\\n=== METEOR ===\")\n",
    "print(\"METEOR Score:\", calculate_meteor(sent1, sent2))\n",
    "\n",
    "print(\"\\n=== Semantic Similarity (STS) ===\")\n",
    "print(\"STS Score:\", calculate_semantic_similarity(sent1, sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello , my name is Teddy.\n",
      "Hi , I ' m Teddy. Nice to meet you !\n",
      "[1] Hello, my name is Teddy.\n",
      "[2] Hi, I'm Teddy. Nice to meet you!\n",
      "[score] 0.25265\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.community.kiwi_tokenizer import KiwiTokenizer\n",
    "\n",
    "# 토크나이저 선언\n",
    "kiwi_tokenizer = KiwiTokenizer()\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "\n",
    "# 토큰화\n",
    "print(kiwi_tokenizer.tokenize(sent1, type=\"sentence\"))\n",
    "print(kiwi_tokenizer.tokenize(sent2, type=\"sentence\"))\n",
    "\n",
    "\n",
    "bleu_score = sentence_bleu(\n",
    "    [kiwi_tokenizer.tokenize(sent1, type=\"sentence\")],\n",
    "    kiwi_tokenizer.tokenize(sent2, type=\"sentence\"),\n",
    ")\n",
    "print(f\"[1] {sent1}\\n[2] {sent2}\\n[score] {bleu_score:.5f}\")\n",
    "print(\"===\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/limseongbeom/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\n",
      "[score] 0.78849\n",
      "============================================================\n",
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 내 이름은 테디입니다. 안녕하세요. 반갑습니다.\n",
      "[score] 0.96800\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn.ensure_loaded()\n",
    "\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "sent1 = \"안녕하세요. 반갑습니다. 내 이름은 테디입니다.\"\n",
    "sent2 = \"안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\"\n",
    "sent3 = \"내 이름은 테디입니다. 안녕하세요. 반갑습니다.\"\n",
    "\n",
    "meteor = meteor_score.meteor_score(\n",
    "    [kiwi_tokenizer.tokenize(sent1, type=\"list\")],\n",
    "    kiwi_tokenizer.tokenize(sent2, type=\"list\"),\n",
    ")\n",
    "\n",
    "print(f\"[1] {sent1}\\n[2] {sent2}\\n[score] {meteor:.5f}\")\n",
    "print(\"===\" * 20)\n",
    "\n",
    "meteor = meteor_score.meteor_score(\n",
    "    [kiwi_tokenizer.tokenize(sent1, type=\"list\")],\n",
    "    kiwi_tokenizer.tokenize(sent3, type=\"list\"),\n",
    ")\n",
    "print(f\"[1] {sent1}\\n[2] {sent3}\\n[score] {meteor:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voltai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
